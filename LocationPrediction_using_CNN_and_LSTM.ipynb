{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "edi7zV_Wj17s",
        "outputId": "f9c3f796-f2a4-4144-8d89-8833e2e40364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ap          1\n",
            "signal      1\n",
            "sequence    1\n",
            "x           1\n",
            "y           1\n",
            "z           1\n",
            "dtype: int64\n",
            "ap          1\n",
            "signal      1\n",
            "sequence    1\n",
            "x           1\n",
            "y           1\n",
            "z           1\n",
            "dtype: int64\n",
            "Epoch 1/100\n",
            "2625/2625 [==============================] - 25s 8ms/step - loss: 142.9108 - val_loss: 83.3905\n",
            "Epoch 2/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 136.3084 - val_loss: 84.8832\n",
            "Epoch 3/100\n",
            "2625/2625 [==============================] - 20s 7ms/step - loss: 134.4453 - val_loss: 84.6812\n",
            "Epoch 4/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 136.0146 - val_loss: 83.4575\n",
            "Epoch 5/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 135.1787 - val_loss: 83.8509\n",
            "Epoch 6/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 135.1736 - val_loss: 81.9492\n",
            "Epoch 7/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 135.4615 - val_loss: 80.7590\n",
            "Epoch 8/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.9355 - val_loss: 80.3380\n",
            "Epoch 9/100\n",
            "2625/2625 [==============================] - 23s 9ms/step - loss: 134.9565 - val_loss: 83.7075\n",
            "Epoch 10/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.8857 - val_loss: 85.0084\n",
            "Epoch 11/100\n",
            "2625/2625 [==============================] - 20s 7ms/step - loss: 135.0890 - val_loss: 85.9092\n",
            "Epoch 12/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.6838 - val_loss: 82.5814\n",
            "Epoch 13/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 133.9642 - val_loss: 84.3602\n",
            "Epoch 14/100\n",
            "2625/2625 [==============================] - 23s 9ms/step - loss: 135.0005 - val_loss: 80.8177\n",
            "Epoch 15/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.8308 - val_loss: 83.0577\n",
            "Epoch 16/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 134.4584 - val_loss: 82.3534\n",
            "Epoch 17/100\n",
            "2625/2625 [==============================] - 23s 9ms/step - loss: 133.9145 - val_loss: 83.3021\n",
            "Epoch 18/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 135.0370 - val_loss: 83.2062\n",
            "Epoch 19/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.9544 - val_loss: 82.3290\n",
            "Epoch 20/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 135.5760 - val_loss: 83.9694\n",
            "Epoch 21/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 134.3472 - val_loss: 81.1791\n",
            "Epoch 22/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 133.8989 - val_loss: 83.7504\n",
            "Epoch 23/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 134.6915 - val_loss: 83.1748\n",
            "Epoch 24/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.0893 - val_loss: 80.8465\n",
            "Epoch 25/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 135.3232 - val_loss: 82.6530\n",
            "Epoch 26/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.5025 - val_loss: 83.9394\n",
            "Epoch 27/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 134.4845 - val_loss: 81.8109\n",
            "Epoch 28/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.8813 - val_loss: 82.1276\n",
            "Epoch 29/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.1651 - val_loss: 82.5258\n",
            "Epoch 30/100\n",
            "2625/2625 [==============================] - 22s 8ms/step - loss: 134.2882 - val_loss: 82.6625\n",
            "Epoch 31/100\n",
            "2625/2625 [==============================] - 20s 7ms/step - loss: 134.6010 - val_loss: 83.0542\n",
            "Epoch 32/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.8369 - val_loss: 81.1112\n",
            "Epoch 33/100\n",
            "2625/2625 [==============================] - 18s 7ms/step - loss: 133.9155 - val_loss: 81.9178\n",
            "Epoch 34/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.8610 - val_loss: 82.3517\n",
            "Epoch 35/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.4082 - val_loss: 82.9567\n",
            "Epoch 36/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 133.7973 - val_loss: 81.5132\n",
            "Epoch 37/100\n",
            "2625/2625 [==============================] - 23s 9ms/step - loss: 134.7608 - val_loss: 82.9091\n",
            "Epoch 38/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 134.9330 - val_loss: 82.4658\n",
            "Epoch 39/100\n",
            "2625/2625 [==============================] - 20s 7ms/step - loss: 134.5367 - val_loss: 81.7456\n",
            "Epoch 40/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.4872 - val_loss: 82.5872\n",
            "Epoch 41/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.8843 - val_loss: 81.1762\n",
            "Epoch 42/100\n",
            "2625/2625 [==============================] - 18s 7ms/step - loss: 134.6127 - val_loss: 81.6977\n",
            "Epoch 43/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 133.4882 - val_loss: 83.5146\n",
            "Epoch 44/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 133.6338 - val_loss: 82.2576\n",
            "Epoch 45/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 133.9170 - val_loss: 81.7736\n",
            "Epoch 46/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.2041 - val_loss: 80.9570\n",
            "Epoch 47/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.2035 - val_loss: 83.3298\n",
            "Epoch 48/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.4050 - val_loss: 83.0041\n",
            "Epoch 49/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.6460 - val_loss: 82.0136\n",
            "Epoch 50/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 135.1077 - val_loss: 81.7035\n",
            "Epoch 51/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.0506 - val_loss: 83.1221\n",
            "Epoch 52/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.2232 - val_loss: 82.0030\n",
            "Epoch 53/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.7452 - val_loss: 81.9735\n",
            "Epoch 54/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.3199 - val_loss: 81.8792\n",
            "Epoch 55/100\n",
            "2625/2625 [==============================] - 18s 7ms/step - loss: 135.3118 - val_loss: 83.8256\n",
            "Epoch 56/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 135.1616 - val_loss: 83.0642\n",
            "Epoch 57/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 133.7302 - val_loss: 82.6490\n",
            "Epoch 58/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 135.3552 - val_loss: 83.0265\n",
            "Epoch 59/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 135.1847 - val_loss: 83.3903\n",
            "Epoch 60/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.8476 - val_loss: 80.4701\n",
            "Epoch 61/100\n",
            "2625/2625 [==============================] - 18s 7ms/step - loss: 134.8355 - val_loss: 83.4846\n",
            "Epoch 62/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.8387 - val_loss: 82.4001\n",
            "Epoch 63/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.0799 - val_loss: 81.7827\n",
            "Epoch 64/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 133.8446 - val_loss: 82.1198\n",
            "Epoch 65/100\n",
            "2625/2625 [==============================] - 20s 7ms/step - loss: 134.0902 - val_loss: 80.6458\n",
            "Epoch 66/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.5033 - val_loss: 84.2915\n",
            "Epoch 67/100\n",
            "2625/2625 [==============================] - 22s 8ms/step - loss: 133.9800 - val_loss: 81.4597\n",
            "Epoch 68/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 135.0148 - val_loss: 81.9297\n",
            "Epoch 69/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.6531 - val_loss: 82.7090\n",
            "Epoch 70/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.4738 - val_loss: 83.7691\n",
            "Epoch 71/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 133.9583 - val_loss: 81.4429\n",
            "Epoch 72/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.3721 - val_loss: 83.3874\n",
            "Epoch 73/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 133.9331 - val_loss: 81.7954\n",
            "Epoch 74/100\n",
            "2625/2625 [==============================] - 22s 8ms/step - loss: 133.5989 - val_loss: 82.8893\n",
            "Epoch 75/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.1192 - val_loss: 81.7760\n",
            "Epoch 76/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.2555 - val_loss: 80.8218\n",
            "Epoch 77/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.5098 - val_loss: 83.0866\n",
            "Epoch 78/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 135.5081 - val_loss: 82.4880\n",
            "Epoch 79/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 134.2087 - val_loss: 84.2315\n",
            "Epoch 80/100\n",
            "2625/2625 [==============================] - 20s 7ms/step - loss: 134.6019 - val_loss: 82.7138\n",
            "Epoch 81/100\n",
            "2625/2625 [==============================] - 23s 9ms/step - loss: 134.4820 - val_loss: 82.4511\n",
            "Epoch 82/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 133.2623 - val_loss: 82.5543\n",
            "Epoch 83/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 133.9432 - val_loss: 81.3885\n",
            "Epoch 84/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.5764 - val_loss: 82.6405\n",
            "Epoch 85/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.3822 - val_loss: 82.5889\n",
            "Epoch 86/100\n",
            "2625/2625 [==============================] - 20s 7ms/step - loss: 134.2926 - val_loss: 81.5098\n",
            "Epoch 87/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.3425 - val_loss: 82.0750\n",
            "Epoch 88/100\n",
            "2625/2625 [==============================] - 23s 9ms/step - loss: 134.8526 - val_loss: 83.3293\n",
            "Epoch 89/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 132.9746 - val_loss: 83.4619\n",
            "Epoch 90/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.2667 - val_loss: 82.9313\n",
            "Epoch 91/100\n",
            "2625/2625 [==============================] - 18s 7ms/step - loss: 133.7790 - val_loss: 81.2267\n",
            "Epoch 92/100\n",
            "2625/2625 [==============================] - 20s 7ms/step - loss: 134.3924 - val_loss: 84.0632\n",
            "Epoch 93/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 133.9024 - val_loss: 81.2839\n",
            "Epoch 94/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 133.9767 - val_loss: 84.8466\n",
            "Epoch 95/100\n",
            "2625/2625 [==============================] - 22s 9ms/step - loss: 134.1323 - val_loss: 82.9469\n",
            "Epoch 96/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.3348 - val_loss: 81.0050\n",
            "Epoch 97/100\n",
            "2625/2625 [==============================] - 21s 8ms/step - loss: 134.5007 - val_loss: 81.4314\n",
            "Epoch 98/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.2545 - val_loss: 82.6384\n",
            "Epoch 99/100\n",
            "2625/2625 [==============================] - 19s 7ms/step - loss: 134.2167 - val_loss: 82.5199\n",
            "Epoch 100/100\n",
            "2625/2625 [==============================] - 20s 8ms/step - loss: 134.3945 - val_loss: 83.1533\n",
            "1125/1125 [==============================] - 3s 2ms/step\n",
            "RMSE: 9.118844846923869\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0b930ff9dfb9>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Inverse transform predictions to the original scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Inverse transform predictions to the original scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Inverse transform actual values to the original scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (35991,3) (5,) (35991,3) "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CSV data file\n",
        "data = pd.read_csv('/content/rssi.csv')\n",
        "\n",
        "# Check for NaN or infinite values in your data\n",
        "print(data.isna().sum())\n",
        "print(data.isin([np.nan, np.inf, -np.inf]).sum())\n",
        "\n",
        "# Handle missing or infinite values (if any)\n",
        "data.dropna(inplace=True)\n",
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Split data into features (RSSI) and labels (x, y, z)\n",
        "X = data[['signal', 'ap']]\n",
        "y = data[['x', 'y', 'z']]\n",
        "\n",
        "# Convert 'ap' column into one-hot encoding\n",
        "X = pd.get_dummies(X, columns=['ap'])\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape the input data to include time steps (1 in this case)\n",
        "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "\n",
        "# Split the data into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(3))  # 3 output neurons for x, y, and z\n",
        "\n",
        "# Add dropout layers for regularization\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# Train the model with more epochs and a smaller batch size\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the root mean squared error (RMSE)\n",
        "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f'RMSE: {rmse}')\n",
        "\n",
        "# Inverse transform predictions to the original scale\n",
        "y_pred = scaler.inverse_transform(y_pred)  # Inverse transform predictions to the original scale\n",
        "y_true = scaler.inverse_transform(y_test)   # Inverse transform actual values to the original scale\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_true, label='Actual', color='blue')\n",
        "plt.plot(y_pred, label='Predicted', color='red')\n",
        "plt.title('Actual vs. Predicted Location')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Location (x, y, z)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Optionally, save the trained model\n",
        "model.save('location_prediction_model.h5')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Dropout\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CSV data file\n",
        "data = pd.read_csv('/content/rssi.csv')\n",
        "\n",
        "# Handle missing or infinite values (if any)\n",
        "data.dropna(inplace=True)\n",
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Split data into features (RSSI) and labels (x, y, z)\n",
        "X = data[['signal', 'ap']]\n",
        "y = data[['x', 'y', 'z']]\n",
        "\n",
        "# Separate numeric and categorical columns\n",
        "numeric_features = ['signal']\n",
        "categorical_features = ['ap']\n",
        "\n",
        "# Create transformers for numeric and categorical features\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(drop='first'))  # Drop first category to avoid multicollinearity\n",
        "])\n",
        "\n",
        "# Use ColumnTransformer to apply different transformers to different columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Define the complete model pipeline\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(X_train_preprocessed.shape[1], X_train_preprocessed.shape[2])))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(3))  # 3 output neurons for x, y, and z\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dropout(0.2))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Split the data into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Apply the preprocessing pipeline to the features\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "# Reshape the input data to include time steps (1 in this case)\n",
        "X_train_preprocessed = X_train_preprocessed.reshape(X_train_preprocessed.shape[0], 1, X_train_preprocessed.shape[1])\n",
        "X_test_preprocessed = X_test_preprocessed.reshape(X_test_preprocessed.shape[0], 1, X_test_preprocessed.shape[1])\n",
        "\n",
        "# Train the model with more epochs and a smaller batch size\n",
        "model.fit(X_train_preprocessed, y_train, epochs=200, batch_size=32, validation_data=(X_test_preprocessed, y_test), verbose=2)\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "y_pred = model.predict(X_test_preprocessed)\n",
        "\n",
        "# Calculate the root mean squared error (RMSE)\n",
        "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f'RMSE: {rmse}')\n",
        "\n",
        "# Optionally, save the trained model\n",
        "model.save('location_prediction_model.h5')\n",
        "\n",
        "# Plot actual vs predicted values (no inverse scaling)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test.values, label='Actual', color='blue')\n",
        "plt.plot(y_pred, label='Predicted', color='red')\n",
        "plt.title('Actual vs. Predicted Location')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Location (x, y, z)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vsCgZcqqyUP4",
        "outputId": "d3669946-ce02-4595-981d-a1aa6282b5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2625/2625 - 34s - loss: 151.7586 - val_loss: 100.8401 - 34s/epoch - 13ms/step\n",
            "Epoch 2/200\n",
            "2625/2625 - 26s - loss: 146.8359 - val_loss: 96.7488 - 26s/epoch - 10ms/step\n",
            "Epoch 3/200\n",
            "2625/2625 - 27s - loss: 142.1207 - val_loss: 92.8496 - 27s/epoch - 10ms/step\n",
            "Epoch 4/200\n",
            "2625/2625 - 25s - loss: 139.7017 - val_loss: 87.3896 - 25s/epoch - 10ms/step\n",
            "Epoch 5/200\n",
            "2625/2625 - 27s - loss: 137.5450 - val_loss: 87.1524 - 27s/epoch - 10ms/step\n",
            "Epoch 6/200\n",
            "2625/2625 - 28s - loss: 137.0067 - val_loss: 84.3254 - 28s/epoch - 11ms/step\n",
            "Epoch 7/200\n",
            "2625/2625 - 25s - loss: 136.4916 - val_loss: 87.2172 - 25s/epoch - 10ms/step\n",
            "Epoch 8/200\n",
            "2625/2625 - 26s - loss: 135.7164 - val_loss: 84.2469 - 26s/epoch - 10ms/step\n",
            "Epoch 9/200\n",
            "2625/2625 - 28s - loss: 134.7743 - val_loss: 84.6935 - 28s/epoch - 11ms/step\n",
            "Epoch 10/200\n",
            "2625/2625 - 25s - loss: 135.2610 - val_loss: 84.4790 - 25s/epoch - 10ms/step\n",
            "Epoch 11/200\n",
            "2625/2625 - 24s - loss: 134.3960 - val_loss: 82.8505 - 24s/epoch - 9ms/step\n",
            "Epoch 12/200\n",
            "2625/2625 - 26s - loss: 135.2704 - val_loss: 81.0463 - 26s/epoch - 10ms/step\n",
            "Epoch 13/200\n",
            "2625/2625 - 26s - loss: 134.5785 - val_loss: 85.3697 - 26s/epoch - 10ms/step\n",
            "Epoch 14/200\n",
            "2625/2625 - 28s - loss: 134.4196 - val_loss: 84.5186 - 28s/epoch - 11ms/step\n",
            "Epoch 15/200\n",
            "2625/2625 - 28s - loss: 135.0518 - val_loss: 84.5877 - 28s/epoch - 11ms/step\n",
            "Epoch 16/200\n",
            "2625/2625 - 29s - loss: 134.5810 - val_loss: 85.7007 - 29s/epoch - 11ms/step\n",
            "Epoch 17/200\n",
            "2625/2625 - 25s - loss: 134.8486 - val_loss: 81.5050 - 25s/epoch - 10ms/step\n",
            "Epoch 18/200\n",
            "2625/2625 - 25s - loss: 135.0551 - val_loss: 81.0725 - 25s/epoch - 10ms/step\n",
            "Epoch 19/200\n",
            "2625/2625 - 26s - loss: 135.2622 - val_loss: 85.1292 - 26s/epoch - 10ms/step\n",
            "Epoch 20/200\n",
            "2625/2625 - 24s - loss: 134.7850 - val_loss: 82.5296 - 24s/epoch - 9ms/step\n",
            "Epoch 21/200\n",
            "2625/2625 - 27s - loss: 134.6133 - val_loss: 84.0833 - 27s/epoch - 10ms/step\n",
            "Epoch 22/200\n",
            "2625/2625 - 28s - loss: 135.2074 - val_loss: 82.9408 - 28s/epoch - 10ms/step\n",
            "Epoch 23/200\n",
            "2625/2625 - 28s - loss: 134.6621 - val_loss: 82.6870 - 28s/epoch - 11ms/step\n",
            "Epoch 24/200\n",
            "2625/2625 - 28s - loss: 134.3052 - val_loss: 80.7971 - 28s/epoch - 11ms/step\n",
            "Epoch 25/200\n",
            "2625/2625 - 27s - loss: 135.3581 - val_loss: 82.8762 - 27s/epoch - 10ms/step\n",
            "Epoch 26/200\n",
            "2625/2625 - 26s - loss: 135.1415 - val_loss: 83.7163 - 26s/epoch - 10ms/step\n",
            "Epoch 27/200\n",
            "2625/2625 - 25s - loss: 135.6044 - val_loss: 84.4200 - 25s/epoch - 10ms/step\n",
            "Epoch 28/200\n",
            "2625/2625 - 25s - loss: 134.5596 - val_loss: 83.8800 - 25s/epoch - 10ms/step\n",
            "Epoch 29/200\n",
            "2625/2625 - 27s - loss: 135.1463 - val_loss: 81.6200 - 27s/epoch - 10ms/step\n",
            "Epoch 30/200\n",
            "2625/2625 - 25s - loss: 135.2297 - val_loss: 83.0322 - 25s/epoch - 9ms/step\n",
            "Epoch 31/200\n",
            "2625/2625 - 27s - loss: 135.1340 - val_loss: 83.4569 - 27s/epoch - 10ms/step\n",
            "Epoch 32/200\n",
            "2625/2625 - 29s - loss: 134.2168 - val_loss: 82.8957 - 29s/epoch - 11ms/step\n",
            "Epoch 33/200\n",
            "2625/2625 - 28s - loss: 135.1817 - val_loss: 82.0037 - 28s/epoch - 11ms/step\n",
            "Epoch 34/200\n",
            "2625/2625 - 27s - loss: 134.7831 - val_loss: 82.3533 - 27s/epoch - 10ms/step\n",
            "Epoch 35/200\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ccc30711ffce>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Train the model with more epochs and a smaller batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_preprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_preprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# Evaluate the model on the testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1740\u001b[0m                         ):\n\u001b[1;32m   1741\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1742\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1743\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    855\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    858\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m       (concrete_function,\n\u001b[1;32m    147\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    149\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1348\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_call_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1457\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1458\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from math import sqrt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your CSV data file for location prediction\n",
        "data = pd.read_csv('/content/rssi.csv')\n",
        "\n",
        "# Check for NaN or infinite values in your data\n",
        "print(data.isna().sum())\n",
        "print(data.isin([np.nan, np.inf, -np.inf]).sum())\n",
        "\n",
        "# Remove rows with missing values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Replace infinite values with NaN\n",
        "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# One-hot encode categorical variables 'signal' and 'ap'\n",
        "data = pd.get_dummies(data, columns=['signal', 'ap'])\n",
        "\n",
        "# Split data into features (RSSI) and labels (x, y, z)\n",
        "X = data.drop(columns=['x', 'y', 'z'])  # Features\n",
        "y = data[['x', 'y', 'z']]  # Target variables\n",
        "\n",
        "# Normalize the features using Min-Max scaling\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Reshape the input data to include time steps (1 in this case)\n",
        "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
        "\n",
        "# Split the data into training (70%) and testing (30%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(3))  # 3 output neurons for x, y, and z\n",
        "\n",
        "# Create a learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "    if epoch < 30:\n",
        "        return 0.001\n",
        "    elif epoch < 60:\n",
        "        return 0.0005\n",
        "    else:\n",
        "        return 0.0001\n",
        "\n",
        "# Compile the model with a learning rate scheduler\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "# Set up early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(patience=20, restore_best_weights=True)\n",
        "\n",
        "# Set up a model checkpoint to save the best model\n",
        "checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "\n",
        "# Learning rate scheduler callback\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=200,  # Adjust as needed\n",
        "    batch_size=32,  # Adjust as needed\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, checkpoint, lr_scheduler],  # Apply early stopping and save the best model\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the root mean squared error (RMSE)\n",
        "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(f'RMSE: {rmse}')\n",
        "\n",
        "# Optionally, save the trained model\n",
        "model.save('location_prediction_model.h5')\n",
        "\n",
        "# Inverse transform predictions to the original scale\n",
        "y_pred = scaler.inverse_transform(y_pred)\n",
        "y_true = scaler.inverse_transform(y_test)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_true, label='Actual', color='blue')\n",
        "plt.plot(y_pred, label='Predicted', color='red')\n",
        "plt.title('Actual vs. Predicted Location')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Location (x, y, z)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "towiDPTO7nKu",
        "outputId": "6fce4bfa-999b-4070-931c-af8f1101f20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ap          1\n",
            "signal      1\n",
            "sequence    1\n",
            "x           1\n",
            "y           1\n",
            "z           1\n",
            "dtype: int64\n",
            "ap          1\n",
            "signal      1\n",
            "sequence    1\n",
            "x           1\n",
            "y           1\n",
            "z           1\n",
            "dtype: int64\n",
            "Epoch 1/200\n",
            "2625/2625 - 20s - loss: 75.3982 - val_loss: 57.5940 - lr: 0.0010 - 20s/epoch - 8ms/step\n",
            "Epoch 2/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2625/2625 - 16s - loss: 59.4019 - val_loss: 53.4413 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 3/200\n",
            "2625/2625 - 17s - loss: 57.0444 - val_loss: 52.8437 - lr: 0.0010 - 17s/epoch - 6ms/step\n",
            "Epoch 4/200\n",
            "2625/2625 - 16s - loss: 56.1494 - val_loss: 52.4172 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 5/200\n",
            "2625/2625 - 16s - loss: 55.4911 - val_loss: 52.2431 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 6/200\n",
            "2625/2625 - 15s - loss: 55.0762 - val_loss: 52.0925 - lr: 0.0010 - 15s/epoch - 6ms/step\n",
            "Epoch 7/200\n",
            "2625/2625 - 16s - loss: 54.7044 - val_loss: 51.8716 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 8/200\n",
            "2625/2625 - 15s - loss: 54.3845 - val_loss: 51.6935 - lr: 0.0010 - 15s/epoch - 6ms/step\n",
            "Epoch 9/200\n",
            "2625/2625 - 18s - loss: 54.2406 - val_loss: 51.6193 - lr: 0.0010 - 18s/epoch - 7ms/step\n",
            "Epoch 10/200\n",
            "2625/2625 - 16s - loss: 53.9579 - val_loss: 51.6859 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 11/200\n",
            "2625/2625 - 17s - loss: 53.7060 - val_loss: 51.2676 - lr: 0.0010 - 17s/epoch - 6ms/step\n",
            "Epoch 12/200\n",
            "2625/2625 - 18s - loss: 53.6255 - val_loss: 51.1788 - lr: 0.0010 - 18s/epoch - 7ms/step\n",
            "Epoch 13/200\n",
            "2625/2625 - 16s - loss: 53.3584 - val_loss: 51.1300 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 14/200\n",
            "2625/2625 - 19s - loss: 53.0840 - val_loss: 51.0699 - lr: 0.0010 - 19s/epoch - 7ms/step\n",
            "Epoch 15/200\n",
            "2625/2625 - 16s - loss: 53.0735 - val_loss: 50.9251 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 16/200\n",
            "2625/2625 - 16s - loss: 52.9248 - val_loss: 50.8592 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 17/200\n",
            "2625/2625 - 16s - loss: 52.8096 - val_loss: 50.8748 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 18/200\n",
            "2625/2625 - 16s - loss: 52.6947 - val_loss: 50.9960 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 19/200\n",
            "2625/2625 - 16s - loss: 52.7669 - val_loss: 50.7554 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 20/200\n",
            "2625/2625 - 16s - loss: 52.6021 - val_loss: 50.8776 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 21/200\n",
            "2625/2625 - 16s - loss: 52.4624 - val_loss: 50.6897 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 22/200\n",
            "2625/2625 - 15s - loss: 52.4762 - val_loss: 51.0384 - lr: 0.0010 - 15s/epoch - 6ms/step\n",
            "Epoch 23/200\n",
            "2625/2625 - 16s - loss: 52.4282 - val_loss: 50.8075 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 24/200\n",
            "2625/2625 - 16s - loss: 52.3199 - val_loss: 50.7169 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 25/200\n",
            "2625/2625 - 17s - loss: 52.2741 - val_loss: 50.7386 - lr: 0.0010 - 17s/epoch - 6ms/step\n",
            "Epoch 26/200\n",
            "2625/2625 - 16s - loss: 52.1433 - val_loss: 50.5633 - lr: 0.0010 - 16s/epoch - 6ms/step\n",
            "Epoch 27/200\n",
            "2625/2625 - 15s - loss: 52.1070 - val_loss: 50.5819 - lr: 0.0010 - 15s/epoch - 6ms/step\n",
            "Epoch 28/200\n",
            "2625/2625 - 18s - loss: 52.1873 - val_loss: 50.5212 - lr: 0.0010 - 18s/epoch - 7ms/step\n",
            "Epoch 29/200\n",
            "2625/2625 - 17s - loss: 52.0529 - val_loss: 50.5197 - lr: 0.0010 - 17s/epoch - 6ms/step\n",
            "Epoch 30/200\n",
            "2625/2625 - 15s - loss: 52.1297 - val_loss: 50.5328 - lr: 0.0010 - 15s/epoch - 6ms/step\n",
            "Epoch 31/200\n",
            "2625/2625 - 15s - loss: 51.8745 - val_loss: 50.4506 - lr: 5.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 32/200\n",
            "2625/2625 - 17s - loss: 51.8176 - val_loss: 50.3455 - lr: 5.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 33/200\n",
            "2625/2625 - 16s - loss: 51.6865 - val_loss: 50.4311 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 34/200\n",
            "2625/2625 - 15s - loss: 51.7066 - val_loss: 50.3494 - lr: 5.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 35/200\n",
            "2625/2625 - 16s - loss: 51.7143 - val_loss: 50.3671 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 36/200\n",
            "2625/2625 - 16s - loss: 51.6403 - val_loss: 50.3484 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 37/200\n",
            "2625/2625 - 15s - loss: 51.6689 - val_loss: 50.3385 - lr: 5.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 38/200\n",
            "2625/2625 - 15s - loss: 51.6198 - val_loss: 50.3900 - lr: 5.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 39/200\n",
            "2625/2625 - 16s - loss: 51.5248 - val_loss: 50.3523 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 40/200\n",
            "2625/2625 - 16s - loss: 51.5737 - val_loss: 50.3636 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 41/200\n",
            "2625/2625 - 16s - loss: 51.5623 - val_loss: 50.2920 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 42/200\n",
            "2625/2625 - 16s - loss: 51.4704 - val_loss: 50.3654 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 43/200\n",
            "2625/2625 - 16s - loss: 51.4630 - val_loss: 50.2675 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 44/200\n",
            "2625/2625 - 16s - loss: 51.5406 - val_loss: 50.2639 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 45/200\n",
            "2625/2625 - 15s - loss: 51.4797 - val_loss: 50.3001 - lr: 5.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 46/200\n",
            "2625/2625 - 16s - loss: 51.6060 - val_loss: 50.3225 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 47/200\n",
            "2625/2625 - 16s - loss: 51.4918 - val_loss: 50.3907 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 48/200\n",
            "2625/2625 - 15s - loss: 51.4094 - val_loss: 50.2362 - lr: 5.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 49/200\n",
            "2625/2625 - 15s - loss: 51.4115 - val_loss: 50.2850 - lr: 5.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 50/200\n",
            "2625/2625 - 16s - loss: 51.4350 - val_loss: 50.2226 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 51/200\n",
            "2625/2625 - 17s - loss: 51.3753 - val_loss: 50.2639 - lr: 5.0000e-04 - 17s/epoch - 7ms/step\n",
            "Epoch 52/200\n",
            "2625/2625 - 16s - loss: 51.3381 - val_loss: 50.2455 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 53/200\n",
            "2625/2625 - 16s - loss: 51.3691 - val_loss: 50.2784 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 54/200\n",
            "2625/2625 - 17s - loss: 51.3743 - val_loss: 50.2697 - lr: 5.0000e-04 - 17s/epoch - 7ms/step\n",
            "Epoch 55/200\n",
            "2625/2625 - 16s - loss: 51.3469 - val_loss: 50.1984 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 56/200\n",
            "2625/2625 - 17s - loss: 51.3228 - val_loss: 50.3080 - lr: 5.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 57/200\n",
            "2625/2625 - 17s - loss: 51.2426 - val_loss: 50.2445 - lr: 5.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 58/200\n",
            "2625/2625 - 16s - loss: 51.2789 - val_loss: 50.2676 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 59/200\n",
            "2625/2625 - 16s - loss: 51.2720 - val_loss: 50.1673 - lr: 5.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 60/200\n",
            "2625/2625 - 17s - loss: 51.3320 - val_loss: 50.2419 - lr: 5.0000e-04 - 17s/epoch - 7ms/step\n",
            "Epoch 61/200\n",
            "2625/2625 - 17s - loss: 51.1613 - val_loss: 50.1447 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 62/200\n",
            "2625/2625 - 16s - loss: 51.0807 - val_loss: 50.1234 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 63/200\n",
            "2625/2625 - 18s - loss: 51.0952 - val_loss: 50.1387 - lr: 1.0000e-04 - 18s/epoch - 7ms/step\n",
            "Epoch 64/200\n",
            "2625/2625 - 16s - loss: 51.0223 - val_loss: 50.1257 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 65/200\n",
            "2625/2625 - 16s - loss: 51.0625 - val_loss: 50.1489 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 66/200\n",
            "2625/2625 - 16s - loss: 51.0649 - val_loss: 50.1604 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 67/200\n",
            "2625/2625 - 15s - loss: 51.0924 - val_loss: 50.1352 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 68/200\n",
            "2625/2625 - 16s - loss: 51.0473 - val_loss: 50.1281 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 69/200\n",
            "2625/2625 - 16s - loss: 51.1154 - val_loss: 50.1458 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 70/200\n",
            "2625/2625 - 17s - loss: 51.0711 - val_loss: 50.1303 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 71/200\n",
            "2625/2625 - 16s - loss: 51.0247 - val_loss: 50.1116 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 72/200\n",
            "2625/2625 - 17s - loss: 51.0990 - val_loss: 50.1263 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 73/200\n",
            "2625/2625 - 16s - loss: 51.0666 - val_loss: 50.1253 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 74/200\n",
            "2625/2625 - 16s - loss: 50.9726 - val_loss: 50.1120 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 75/200\n",
            "2625/2625 - 15s - loss: 51.0369 - val_loss: 50.1179 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 76/200\n",
            "2625/2625 - 18s - loss: 51.0424 - val_loss: 50.1118 - lr: 1.0000e-04 - 18s/epoch - 7ms/step\n",
            "Epoch 77/200\n",
            "2625/2625 - 16s - loss: 51.0714 - val_loss: 50.1045 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 78/200\n",
            "2625/2625 - 15s - loss: 50.9824 - val_loss: 50.1012 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 79/200\n",
            "2625/2625 - 17s - loss: 51.0138 - val_loss: 50.1092 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 80/200\n",
            "2625/2625 - 16s - loss: 51.0814 - val_loss: 50.1139 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 81/200\n",
            "2625/2625 - 15s - loss: 51.0061 - val_loss: 50.1141 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 82/200\n",
            "2625/2625 - 15s - loss: 50.9986 - val_loss: 50.1062 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 83/200\n",
            "2625/2625 - 17s - loss: 51.0675 - val_loss: 50.1278 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 84/200\n",
            "2625/2625 - 16s - loss: 50.9922 - val_loss: 50.0911 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 85/200\n",
            "2625/2625 - 16s - loss: 50.9878 - val_loss: 50.1239 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 86/200\n",
            "2625/2625 - 16s - loss: 51.0194 - val_loss: 50.1285 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 87/200\n",
            "2625/2625 - 17s - loss: 51.0532 - val_loss: 50.0978 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 88/200\n",
            "2625/2625 - 16s - loss: 50.9768 - val_loss: 50.1089 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 89/200\n",
            "2625/2625 - 16s - loss: 51.0535 - val_loss: 50.1221 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 90/200\n",
            "2625/2625 - 17s - loss: 51.0529 - val_loss: 50.1015 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 91/200\n",
            "2625/2625 - 17s - loss: 50.9692 - val_loss: 50.0942 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 92/200\n",
            "2625/2625 - 17s - loss: 51.0074 - val_loss: 50.1027 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 93/200\n",
            "2625/2625 - 17s - loss: 50.9793 - val_loss: 50.0889 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 94/200\n",
            "2625/2625 - 15s - loss: 50.9930 - val_loss: 50.1391 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 95/200\n",
            "2625/2625 - 16s - loss: 50.9561 - val_loss: 50.1083 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 96/200\n",
            "2625/2625 - 18s - loss: 50.9747 - val_loss: 50.0902 - lr: 1.0000e-04 - 18s/epoch - 7ms/step\n",
            "Epoch 97/200\n",
            "2625/2625 - 16s - loss: 51.0531 - val_loss: 50.1276 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 98/200\n",
            "2625/2625 - 15s - loss: 50.9263 - val_loss: 50.0836 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 99/200\n",
            "2625/2625 - 19s - loss: 50.9582 - val_loss: 50.1277 - lr: 1.0000e-04 - 19s/epoch - 7ms/step\n",
            "Epoch 100/200\n",
            "2625/2625 - 16s - loss: 50.9234 - val_loss: 50.0812 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 101/200\n",
            "2625/2625 - 16s - loss: 50.9980 - val_loss: 50.0897 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 102/200\n",
            "2625/2625 - 17s - loss: 50.9597 - val_loss: 50.0938 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 103/200\n",
            "2625/2625 - 15s - loss: 50.9532 - val_loss: 50.1106 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 104/200\n",
            "2625/2625 - 16s - loss: 50.9856 - val_loss: 50.1067 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 105/200\n",
            "2625/2625 - 18s - loss: 50.9814 - val_loss: 50.0825 - lr: 1.0000e-04 - 18s/epoch - 7ms/step\n",
            "Epoch 106/200\n",
            "2625/2625 - 16s - loss: 50.8909 - val_loss: 50.0778 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 107/200\n",
            "2625/2625 - 16s - loss: 50.9702 - val_loss: 50.0779 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 108/200\n",
            "2625/2625 - 19s - loss: 50.9064 - val_loss: 50.0795 - lr: 1.0000e-04 - 19s/epoch - 7ms/step\n",
            "Epoch 109/200\n",
            "2625/2625 - 17s - loss: 50.8610 - val_loss: 50.0737 - lr: 1.0000e-04 - 17s/epoch - 7ms/step\n",
            "Epoch 110/200\n",
            "2625/2625 - 19s - loss: 50.8586 - val_loss: 50.0934 - lr: 1.0000e-04 - 19s/epoch - 7ms/step\n",
            "Epoch 111/200\n",
            "2625/2625 - 16s - loss: 50.9962 - val_loss: 50.1149 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 112/200\n",
            "2625/2625 - 16s - loss: 50.9764 - val_loss: 50.0784 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 113/200\n",
            "2625/2625 - 17s - loss: 50.9519 - val_loss: 50.0776 - lr: 1.0000e-04 - 17s/epoch - 7ms/step\n",
            "Epoch 114/200\n",
            "2625/2625 - 15s - loss: 50.9236 - val_loss: 50.0836 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 115/200\n",
            "2625/2625 - 15s - loss: 50.9049 - val_loss: 50.0861 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 116/200\n",
            "2625/2625 - 17s - loss: 51.0260 - val_loss: 50.0923 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 117/200\n",
            "2625/2625 - 16s - loss: 50.9382 - val_loss: 50.1019 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 118/200\n",
            "2625/2625 - 16s - loss: 50.9392 - val_loss: 50.0915 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 119/200\n",
            "2625/2625 - 16s - loss: 50.9948 - val_loss: 50.1007 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 120/200\n",
            "2625/2625 - 17s - loss: 50.9149 - val_loss: 50.0808 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 121/200\n",
            "2625/2625 - 16s - loss: 50.8801 - val_loss: 50.0787 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 122/200\n",
            "2625/2625 - 16s - loss: 50.9497 - val_loss: 50.0924 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 123/200\n",
            "2625/2625 - 16s - loss: 50.9539 - val_loss: 50.1048 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 124/200\n",
            "2625/2625 - 16s - loss: 50.9110 - val_loss: 50.0725 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 125/200\n",
            "2625/2625 - 16s - loss: 50.9331 - val_loss: 50.0776 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 126/200\n",
            "2625/2625 - 15s - loss: 50.9777 - val_loss: 50.0661 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 127/200\n",
            "2625/2625 - 16s - loss: 50.8569 - val_loss: 50.0741 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 128/200\n",
            "2625/2625 - 16s - loss: 50.8628 - val_loss: 50.0745 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 129/200\n",
            "2625/2625 - 15s - loss: 50.8891 - val_loss: 50.0633 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 130/200\n",
            "2625/2625 - 16s - loss: 50.8674 - val_loss: 50.0696 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 131/200\n",
            "2625/2625 - 19s - loss: 50.9183 - val_loss: 50.0719 - lr: 1.0000e-04 - 19s/epoch - 7ms/step\n",
            "Epoch 132/200\n",
            "2625/2625 - 16s - loss: 50.9015 - val_loss: 50.0766 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 133/200\n",
            "2625/2625 - 16s - loss: 50.9074 - val_loss: 50.0660 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 134/200\n",
            "2625/2625 - 15s - loss: 50.9977 - val_loss: 50.0848 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 135/200\n",
            "2625/2625 - 16s - loss: 50.8975 - val_loss: 50.0872 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 136/200\n",
            "2625/2625 - 15s - loss: 50.9213 - val_loss: 50.0684 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 137/200\n",
            "2625/2625 - 16s - loss: 50.9494 - val_loss: 50.0602 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 138/200\n",
            "2625/2625 - 19s - loss: 50.8441 - val_loss: 50.0672 - lr: 1.0000e-04 - 19s/epoch - 7ms/step\n",
            "Epoch 139/200\n",
            "2625/2625 - 16s - loss: 50.8796 - val_loss: 50.0834 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 140/200\n",
            "2625/2625 - 15s - loss: 50.9529 - val_loss: 50.0919 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 141/200\n",
            "2625/2625 - 16s - loss: 50.8832 - val_loss: 50.0722 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 142/200\n",
            "2625/2625 - 16s - loss: 50.8965 - val_loss: 50.0659 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 143/200\n",
            "2625/2625 - 16s - loss: 50.8763 - val_loss: 50.0690 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 144/200\n",
            "2625/2625 - 16s - loss: 50.9557 - val_loss: 50.0695 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 145/200\n",
            "2625/2625 - 18s - loss: 50.8119 - val_loss: 50.0620 - lr: 1.0000e-04 - 18s/epoch - 7ms/step\n",
            "Epoch 146/200\n",
            "2625/2625 - 16s - loss: 50.8718 - val_loss: 50.0599 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 147/200\n",
            "2625/2625 - 15s - loss: 50.9414 - val_loss: 50.0734 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 148/200\n",
            "2625/2625 - 17s - loss: 50.8507 - val_loss: 50.0918 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 149/200\n",
            "2625/2625 - 16s - loss: 50.8803 - val_loss: 50.0551 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 150/200\n",
            "2625/2625 - 16s - loss: 50.8279 - val_loss: 50.0875 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 151/200\n",
            "2625/2625 - 16s - loss: 50.8917 - val_loss: 50.0516 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 152/200\n",
            "2625/2625 - 16s - loss: 50.8214 - val_loss: 50.0752 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 153/200\n",
            "2625/2625 - 16s - loss: 50.9455 - val_loss: 50.0547 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 154/200\n",
            "2625/2625 - 16s - loss: 50.8181 - val_loss: 50.0577 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 155/200\n",
            "2625/2625 - 16s - loss: 50.8544 - val_loss: 50.0629 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 156/200\n",
            "2625/2625 - 15s - loss: 50.7891 - val_loss: 50.0574 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 157/200\n",
            "2625/2625 - 15s - loss: 50.8602 - val_loss: 50.0614 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 158/200\n",
            "2625/2625 - 15s - loss: 50.8641 - val_loss: 50.0782 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 159/200\n",
            "2625/2625 - 16s - loss: 50.7699 - val_loss: 50.0692 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 160/200\n",
            "2625/2625 - 16s - loss: 50.8347 - val_loss: 50.0632 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 161/200\n",
            "2625/2625 - 16s - loss: 50.8560 - val_loss: 50.0908 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 162/200\n",
            "2625/2625 - 16s - loss: 50.8278 - val_loss: 50.0574 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 163/200\n",
            "2625/2625 - 16s - loss: 50.8843 - val_loss: 50.0414 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 164/200\n",
            "2625/2625 - 18s - loss: 50.7882 - val_loss: 50.0639 - lr: 1.0000e-04 - 18s/epoch - 7ms/step\n",
            "Epoch 165/200\n",
            "2625/2625 - 19s - loss: 50.8059 - val_loss: 50.0563 - lr: 1.0000e-04 - 19s/epoch - 7ms/step\n",
            "Epoch 166/200\n",
            "2625/2625 - 16s - loss: 50.8318 - val_loss: 50.0593 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 167/200\n",
            "2625/2625 - 15s - loss: 50.8328 - val_loss: 50.0890 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 168/200\n",
            "2625/2625 - 15s - loss: 50.8133 - val_loss: 50.0478 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 169/200\n",
            "2625/2625 - 17s - loss: 50.8051 - val_loss: 50.0658 - lr: 1.0000e-04 - 17s/epoch - 7ms/step\n",
            "Epoch 170/200\n",
            "2625/2625 - 16s - loss: 50.8247 - val_loss: 50.0797 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 171/200\n",
            "2625/2625 - 20s - loss: 50.9037 - val_loss: 50.0650 - lr: 1.0000e-04 - 20s/epoch - 7ms/step\n",
            "Epoch 172/200\n",
            "2625/2625 - 16s - loss: 50.8374 - val_loss: 50.0679 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 173/200\n",
            "2625/2625 - 15s - loss: 50.8270 - val_loss: 50.0493 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 174/200\n",
            "2625/2625 - 16s - loss: 50.7796 - val_loss: 50.0459 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 175/200\n",
            "2625/2625 - 16s - loss: 50.8026 - val_loss: 50.0599 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 176/200\n",
            "2625/2625 - 16s - loss: 50.8602 - val_loss: 50.0460 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 177/200\n",
            "2625/2625 - 16s - loss: 50.8411 - val_loss: 50.0400 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 178/200\n",
            "2625/2625 - 17s - loss: 50.8207 - val_loss: 50.0396 - lr: 1.0000e-04 - 17s/epoch - 7ms/step\n",
            "Epoch 179/200\n",
            "2625/2625 - 16s - loss: 50.8299 - val_loss: 50.0441 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 180/200\n",
            "2625/2625 - 16s - loss: 50.8315 - val_loss: 50.0449 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 181/200\n",
            "2625/2625 - 16s - loss: 50.7972 - val_loss: 50.0510 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 182/200\n",
            "2625/2625 - 18s - loss: 50.7937 - val_loss: 50.0334 - lr: 1.0000e-04 - 18s/epoch - 7ms/step\n",
            "Epoch 183/200\n",
            "2625/2625 - 16s - loss: 50.8510 - val_loss: 50.0503 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 184/200\n",
            "2625/2625 - 16s - loss: 50.8234 - val_loss: 50.0374 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 185/200\n",
            "2625/2625 - 16s - loss: 50.8564 - val_loss: 50.0675 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 186/200\n",
            "2625/2625 - 16s - loss: 50.8276 - val_loss: 50.0620 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 187/200\n",
            "2625/2625 - 16s - loss: 50.8021 - val_loss: 50.0577 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 188/200\n",
            "2625/2625 - 17s - loss: 50.7412 - val_loss: 50.0409 - lr: 1.0000e-04 - 17s/epoch - 6ms/step\n",
            "Epoch 189/200\n",
            "2625/2625 - 15s - loss: 50.7633 - val_loss: 50.0454 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 190/200\n",
            "2625/2625 - 19s - loss: 50.8235 - val_loss: 50.0580 - lr: 1.0000e-04 - 19s/epoch - 7ms/step\n",
            "Epoch 191/200\n",
            "2625/2625 - 16s - loss: 50.8284 - val_loss: 50.0334 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 192/200\n",
            "2625/2625 - 16s - loss: 50.8259 - val_loss: 50.0344 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 193/200\n",
            "2625/2625 - 16s - loss: 50.8030 - val_loss: 50.0404 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 194/200\n",
            "2625/2625 - 16s - loss: 50.7875 - val_loss: 50.0679 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 195/200\n",
            "2625/2625 - 16s - loss: 50.8366 - val_loss: 50.0506 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 196/200\n",
            "2625/2625 - 16s - loss: 50.7778 - val_loss: 50.0515 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 197/200\n",
            "2625/2625 - 16s - loss: 50.8412 - val_loss: 50.0527 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 198/200\n",
            "2625/2625 - 16s - loss: 50.8081 - val_loss: 50.0254 - lr: 1.0000e-04 - 16s/epoch - 6ms/step\n",
            "Epoch 199/200\n",
            "2625/2625 - 15s - loss: 50.8324 - val_loss: 50.0409 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "Epoch 200/200\n",
            "2625/2625 - 15s - loss: 50.7956 - val_loss: 50.0522 - lr: 1.0000e-04 - 15s/epoch - 6ms/step\n",
            "1125/1125 [==============================] - 4s 3ms/step\n",
            "RMSE: 7.0747595443918465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ef360331f42c>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Inverse transform predictions to the original scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    539\u001b[0m         )\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (35991,3) (66,) (35991,3) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_pred = model.predict(X_test_reshaped)\n",
        "\n",
        "# Convert X_test_reshaped to float32 (if not already)\n",
        "X_test_reshaped = X_test_reshaped.astype(np.float32)\n",
        "\n",
        "# Inverse transform predictions to original scale\n",
        "y_pred = scaler_y.inverse_transform(y_pred)\n",
        "\n",
        "# Convert y_test (if needed) to float32 before inverse transforming\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "# Inverse transform actual values to original scale\n",
        "y_true = scaler_y.inverse_transform(y_test)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_true, label='Actual', color='blue')\n",
        "plt.plot(y_pred, label='Predicted', color='red')\n",
        "plt.title('Actual vs. Predicted Location')\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Location (x, y, z)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "NpgnIKK5LcwS",
        "outputId": "195baec3-31a5-452b-ab52-45d4f18b8aaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-316a44532fe7>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert X_test_reshaped to float32 (if not already)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_test_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test_reshaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xOjdnKXkLjZq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}